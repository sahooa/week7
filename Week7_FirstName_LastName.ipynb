{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> UCI - I&C SCI_X426.53 (Fall2017) </h1>\n",
    "**Course Instructors:** <br>\n",
    "Ted Pham (email:tedp@uci.edu) <br>\n",
    "William Henry  (will@henryanalytics.com)\n",
    "\n",
    "\n",
    "\n",
    "<h2>Week7 Assignment</h2>\n",
    "\n",
    "\n",
    "---\n",
    "This is an individual assignment, but feel free to exchange ideas on the forum.\n",
    "__Name:__  Your Name Here<br>\n",
    "__Email:__  peter_anteater@uci.edu   \n",
    "__Week:__   7\n",
    "\n",
    "__Due Time:__ Friday Dec 21 10am PST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook, we will go over examples of running Spark and do some exercises\n",
    "    1. KDnuggets Tutorial\n",
    "    2. WordCount Exercise\n",
    "    3. K-means Example\n",
    "    4. K-means Exercise\n",
    "\n",
    "This notebook was tested on AWS EC2 jupyter interface using UCI AMI provided in previous weeks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import SparkContext from pyspark\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try Week6 code\n",
    "from operator import add\n",
    "rdd1 = sc.parallelize([(\"a\",1),(\"b\",1),(\"a\",1)])\n",
    "sorted(rdd1.reduceByKey(add).collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. KDnuggets Spark Tutorial\n",
    "\n",
    "kdnuggets.com has an excellent tutorial on Spark and Python\n",
    "https://www.kdnuggets.com/2015/11/introduction-spark-python.html\n",
    "\n",
    "Due to copyrights, please read the tutorial and we follow along a slightly modified code here.\n",
    "\n",
    "Starting with downloading the daily_show_guests.csv. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  749k    0  749k    0     0   749k      0 --:--:-- --:--:-- --:--:-- 2507k\n"
     ]
    }
   ],
   "source": [
    "!curl -L https://github.com/fivethirtyeight/data/blob/master/daily-show-guests/daily_show_guests.csv -o daily.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YEAR,GoogleKnowlege_Occupation,Show,Group,Raw_Guest_List\r\n",
      "1999,actor,1/11/99,Acting,Michael J. Fox\r\n",
      "1999,Comedian,1/12/99,Comedy,Sandra Bernhard\r\n",
      "1999,television actress,1/13/99,Acting,Tracey Ullman\r\n",
      "1999,film actress,1/14/99,Acting,Gillian Anderson\r\n",
      "1999,actor,1/18/99,Acting,David Alan Grier\r\n",
      "1999,actor,1/19/99,Acting,William Baldwin\r\n",
      "1999,Singer-lyricist,1/20/99,Musician,Michael Stipe\r\n",
      "1999,model,1/21/99,Media,Carmen Electra\r\n",
      "1999,actor,1/25/99,Acting,Matthew Lillard\r\n"
     ]
    }
   ],
   "source": [
    "!head -10 daily.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YEAR,GoogleKnowlege_Occupation,Show,Group,Raw_Guest_List',\n",
       " '1999,actor,1/11/99,Acting,Michael J. Fox',\n",
       " '1999,Comedian,1/12/99,Comedy,Sandra Bernhard',\n",
       " '1999,television actress,1/13/99,Acting,Tracey Ullman',\n",
       " '1999,film actress,1/14/99,Acting,Gillian Anderson']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-get the first 5 elements from the raw RDD\n",
    "raw = sc.textFile(\"daily.csv\")\n",
    "raw.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['YEAR', 'GoogleKnowlege_Occupation', 'Show', 'Group', 'Raw_Guest_List'],\n",
       " ['1999', 'actor', '1/11/99', 'Acting', 'Michael J. Fox'],\n",
       " ['1999', 'Comedian', '1/12/99', 'Comedy', 'Sandra Bernhard'],\n",
       " ['1999', 'television actress', '1/13/99', 'Acting', 'Tracey Ullman'],\n",
       " ['1999', 'film actress', '1/14/99', 'Acting', 'Gillian Anderson']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-split elements by commas\n",
    "daily = raw.map(lambda line: line.split(','))\n",
    "daily.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[14] at RDD at PythonRDD.scala:48\n"
     ]
    }
   ],
   "source": [
    "# Aggregate total count of visitors per year\n",
    "tally = daily.map(lambda x: (x[0], 1))\\\n",
    "             .reduceByKey(lambda x,y: x+y)\n",
    "print(tally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('YEAR', 1),\n",
       " ('2002', 159),\n",
       " ('2003', 166),\n",
       " ('2004', 164),\n",
       " ('2007', 141),\n",
       " ('2010', 165),\n",
       " ('2011', 163),\n",
       " ('2012', 164),\n",
       " ('2013', 166),\n",
       " ('2014', 163),\n",
       " ('2015', 100),\n",
       " ('1999', 166),\n",
       " ('2000', 169),\n",
       " ('2001', 157),\n",
       " ('2005', 162),\n",
       " ('2006', 161),\n",
       " ('2008', 164),\n",
       " ('2009', 163)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# because Spark is lazy we need to perform an action on the RDD\n",
    "tally.take(tally.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1 (5pts)**\n",
    "Sort the tally by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Student Code#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actor', 596),\n",
       " ('film actress', 21),\n",
       " ('model', 9),\n",
       " ('stand-up comedian', 44),\n",
       " ('actress', 271)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter out empty profession, make profession lowercase\n",
    "# and get profession count for the visitors\n",
    "filtered_daily = daily.filter(lambda line: line[1] != '') \\\n",
    "                      .map(lambda line: (line[1].lower(), 1))\\\n",
    "                      .reduceByKey(lambda x,y: x+y)\n",
    "        \n",
    "# get first 5 profession counts\n",
    "filtered_daily.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 (5pts)** Get all professions and counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-Student Code-#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. WordCount Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A CHRISTMAS CAROL\r\n",
      "\r\n",
      "IN PROSE\r\n",
      "BEING\r\n",
      "A Ghost Story of Christmas\r\n",
      "\r\n",
      "by Charles Dickens\r\n",
      "\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# We will perform word count using Spark with Christmas Carol Text\n",
    "# If you are working with the notebook on your local computer instead \n",
    "# of EC2 you can download the text from https://www.gutenberg.org/files/46/46-h/46-h.htm\n",
    "# We look at first 10 lines\n",
    "!head -10 christmas.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total Word Count from Christmas Carol***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4208\n",
      "A CHRISTMAS CAROL\n",
      "['A CHRISTMAS CAROL', '', 'IN PROSE', 'BEING', 'A Ghost Story of Christmas', '', 'by Charles Dickens', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "# Create a textfile RDD\n",
    "christmas = sc.textFile(\"christmas.txt\")\n",
    "\n",
    "#simple actions on the RDD\n",
    "print(christmas.count())\n",
    "print(christmas.first())\n",
    "print(christmas.take(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 (10 pts)**\n",
    "Use Spark, get the 20 most common \"lowercased\" words (don't count stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Pseudocode\n",
    "\n",
    "1. Define a function \"findWord\" that takes a line as input and return the words & their counts (if stopwords, don't count the word)\n",
    "\n",
    "2. define a count RDD as:\n",
    "    a. flatmap(find,Word)\n",
    "    b. aggregate by key, add count\n",
    "    c. switch ke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1745, 'the')\n",
      "(1128, 'and')\n",
      "(775, 'of')\n",
      "(764, 'a')\n",
      "(759, 'to')\n",
      "(577, 'in')\n",
      "(557, 'it')\n",
      "(496, 'he')\n",
      "(428, 'was')\n",
      "(420, 'his')\n",
      "(367, 'i')\n",
      "(365, 'that')\n",
      "(362, 'scrooge')\n",
      "(314, 'with')\n",
      "(314, 'you')\n",
      "(246, 's')\n",
      "(239, 'as')\n",
      "(222, 'for')\n",
      "(221, 'said')\n",
      "(205, 'had')\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Question 4 (10 pts)**\n",
    "Use Spark, get the 20 least common \"lowercased\" words (don't count stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
